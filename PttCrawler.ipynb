{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PttCrawler.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fxxxtsao/Crawler/blob/master/PttCrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJ5odsYLWMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import json \n",
        "import re\n",
        "import time\n",
        "def get_pages():\n",
        "    pages = []\n",
        "    n = 39200\n",
        "    while True:\n",
        "        url = 'https://www.ptt.cc/bbs/Gossiping/index'+str(n)+'.html'\n",
        "        headers = {'user-agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'}\n",
        "        res = requests.get(url, headers=headers, cookies={'over18': '1'}).text\n",
        "        soup = bs(res, 'lxml')\n",
        "        lessons = soup.select('a' )\n",
        "        if lessons == []:\n",
        "            break\n",
        "        else :\n",
        "            pages.append(url)\n",
        "            n += 1\n",
        "            #print(url)\n",
        "            time.sleep(1)\n",
        "    return pages\n",
        "\n",
        "def get_article_web():\n",
        "    res = requests.get(pages[0], headers=headers, cookies={'over18': '1'}).text\n",
        "    soup = bs(res, 'lxml')\n",
        "    article = soup.find_all('div', class_='title')\n",
        "    data = []\n",
        "\n",
        "    for item in article:\n",
        "        try:\n",
        "            data.append('https://www.ptt.cc/'+item.find('a').get('href'))\n",
        "        except :continue\n",
        "        \n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDARDGruQ7bn",
        "colab_type": "text"
      },
      "source": [
        "### 推文數　pages[0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPNttNgm-l-z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0836e0cd-a5ec-49f3-9aab-507e1a5ee07d"
      },
      "source": [
        "def get_article_push():\n",
        "    res = requests.get(pages[0], headers=headers, cookies={'over18': '1'}).text\n",
        "    soup = bs(res, 'lxml')\n",
        "    article_push = soup.find_all('div', class_='nrec')\n",
        "    push = []\n",
        "    #print(article)\n",
        "    for pu in article_push:\n",
        "        try:\n",
        "            push.append(pu.find('span').text)\n",
        "        except :continue\n",
        "    return push\n",
        "\n",
        "\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['37', '2', '爆', '9', '7', '1', '14', '14', '18', '9', '7', '4', '7', '爆', '1', '8']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4NL4XseTaS1",
        "colab_type": "text"
      },
      "source": [
        "### 文章略標　pages[0] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwEXZZgjTZJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article_title():\n",
        "    res = requests.get(pages[0], headers=headers, cookies={'over18': '1'}).text\n",
        "    soup = bs(res, 'lxml')\n",
        "    article = soup.find_all('div', class_='title')\n",
        "    title = []\n",
        "    for it in article:\n",
        "        try:\n",
        "            title.append(it.find('a').text)\n",
        "        except :continue\n",
        "        \n",
        "    return title"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}